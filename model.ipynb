{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jaguar Re-Identification\n",
    "\n",
    "## Score: .346"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import timm\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFIG\n",
    "# =============================================================================\n",
    "class CFG:\n",
    "    data_dir = Path('jaguar-re-id')\n",
    "    test_csv = data_dir / 'test.csv'\n",
    "    test_dir = data_dir / 'test' / 'test'\n",
    "    \n",
    "    backbone = 'hf-hub:BVRA/MegaDescriptor-L-384'\n",
    "    image_size = 384\n",
    "    batch_size = 8\n",
    "    use_tta = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test pairs: 137270 | Unique images: 371\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# DATA\n",
    "# =============================================================================\n",
    "test_df = pd.read_csv(CFG.test_csv)\n",
    "unique_images = sorted(set(test_df['query_image']) | set(test_df['gallery_image']))\n",
    "print(f\"Test pairs: {len(test_df)} | Unique images: {len(unique_images)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATASET\n",
    "# =============================================================================\n",
    "def get_transforms(flip=False):\n",
    "    t = [A.LongestMaxSize(max_size=CFG.image_size),\n",
    "         A.PadIfNeeded(CFG.image_size, CFG.image_size, border_mode=0)]\n",
    "    if flip:\n",
    "        t.append(A.HorizontalFlip(p=1.0))\n",
    "    t.extend([A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)), ToTensorV2()])\n",
    "    return A.Compose(t)\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, image_files, img_dir, transform):\n",
    "        self.image_files = image_files\n",
    "        self.img_dir = Path(img_dir)\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        fname = self.image_files[idx]\n",
    "        img = np.array(Image.open(self.img_dir / fname).convert('RGB'))\n",
    "        return self.transform(image=img)['image'], fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading hf-hub:BVRA/MegaDescriptor-L-384...\n",
      "Embedding: 1536 | Params: 195,198,516\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# MODEL\n",
    "# =============================================================================\n",
    "print(f\"Loading {CFG.backbone}...\")\n",
    "model = timm.create_model(CFG.backbone, pretrained=True, num_classes=0).to(device).eval()\n",
    "print(f\"Embedding: {model.num_features} | Params: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TTA: original + flip\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39a81294842d4d9b983d1836f80d47d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting:   0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c03b93f5b58343098c38cdcd044b829c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting:   0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# INFERENCE\n",
    "# =============================================================================\n",
    "@torch.no_grad()\n",
    "def extract_embeddings(transform):\n",
    "    loader = DataLoader(TestDataset(unique_images, CFG.test_dir, transform), \n",
    "                        batch_size=CFG.batch_size, shuffle=False, num_workers=0)\n",
    "    emb_dict = {}\n",
    "    for images, fnames in tqdm(loader, desc='Extracting'):\n",
    "        emb = F.normalize(model(images.to(device)), p=2, dim=1)\n",
    "        for f, e in zip(fnames, emb):\n",
    "            emb_dict[f] = e.cpu().numpy()\n",
    "    return emb_dict\n",
    "\n",
    "if CFG.use_tta:\n",
    "    print(\"TTA: original + flip\")\n",
    "    emb_orig = extract_embeddings(get_transforms(flip=False))\n",
    "    emb_flip = extract_embeddings(get_transforms(flip=True))\n",
    "    embeddings = {f: (emb_orig[f] + emb_flip[f]) / np.linalg.norm(emb_orig[f] + emb_flip[f]) for f in unique_images}\n",
    "else:\n",
    "    embeddings = extract_embeddings(get_transforms(flip=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing similarities...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f387b91f9cc49bb806dc084f7846433",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/137270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved | Mean: 0.5826 | Std: 0.0638\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SUBMISSION\n",
    "# =============================================================================\n",
    "print(\"Computing similarities...\")\n",
    "sims = [np.dot(embeddings[r['query_image']], embeddings[r['gallery_image']]) for _, r in tqdm(test_df.iterrows(), total=len(test_df))]\n",
    "sims = [(s + 1) / 2 for s in sims]\n",
    "\n",
    "submission = pd.DataFrame({'row_id': test_df['row_id'], 'similarity': sims})\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(f\"Saved | Mean: {submission['similarity'].mean():.4f} | Std: {submission['similarity'].std():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
